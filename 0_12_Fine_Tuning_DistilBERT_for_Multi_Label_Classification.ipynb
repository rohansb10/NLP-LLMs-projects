{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTR6kCYNKH0j"
      },
      "outputs": [],
      "source": [
        "!pip install -U accelerate\n",
        "!pip install -U transformers\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/laxmimerit/All-CSV-ML-Data-Files-Download/master/IMDB-Movie-Data.csv\", usecols=[\"Description\", \"Genre\"])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "xy8GuCrGNpZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Genre'] = df['Genre'].str.split(',')\n",
        "genre_counts = [g for gen in df['Genre'] for g in gen]\n",
        "pd.Series(genre_counts).value_counts()"
      ],
      "metadata": {
        "id": "iqWztJqhNv-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "multilabel = MultiLabelBinarizer()\n",
        "\n",
        "labels = multilabel.fit_transform(df['Genre']).astype('float32')\n",
        "\n",
        "texts = df['Description'].tolist()\n",
        "labels\n",
        "texts[:5]"
      ],
      "metadata": {
        "id": "RDB5GrT9NzY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import DistilBertTokenizer, AutoTokenizer\n",
        "from transformers import DistilBertForSequenceClassification, AutoModelForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels,\n",
        "                                                                    test_size=0.2, random_state=42)\n",
        "checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(checkpoint)\n",
        "model = DistilBertForSequenceClassification.from_pretrained(checkpoint, num_labels=len(labels[0]),\n",
        "                                                            problem_type=\"multi_label_classification\")"
      ],
      "metadata": {
        "id": "148jr2XkN4C4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets build custom dataset\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, texts, labels, tokenizer, max_len=128):\n",
        "    self.texts = texts\n",
        "    self.labels = labels\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.texts)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    text = str(self.texts[idx])\n",
        "    label = torch.tensor(self.labels[idx])\n",
        "\n",
        "    encoding = self.tokenizer(text, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors='pt')\n",
        "\n",
        "    return {\n",
        "        'input_ids': encoding['input_ids'].flatten(),\n",
        "        'attention_mask': encoding['attention_mask'].flatten(),\n",
        "        'labels': label\n",
        "    }\n",
        "train_dataset = CustomDataset(train_texts, train_labels, tokenizer)\n",
        "val_dataset = CustomDataset(val_texts, val_labels, tokenizer)\n",
        "# val_dataset[0]\n",
        "# Multi-Label Classification Evaluation Metrics\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score, f1_score, hamming_loss\n",
        "from transformers import EvalPrediction\n",
        "import torch\n",
        "\n",
        "\n",
        "def multi_labels_metrics(predictions, labels, threshold=0.3):\n",
        "  sigmoid = torch.nn.Sigmoid()\n",
        "  probs = sigmoid(torch.Tensor(predictions))\n",
        "\n",
        "  y_pred = np.zeros(probs.shape)\n",
        "  y_pred[np.where(probs>=threshold)] = 1\n",
        "  y_true = labels\n",
        "\n",
        "  f1 = f1_score(y_true, y_pred, average = 'macro')\n",
        "  roc_auc = roc_auc_score(y_true, y_pred, average = 'macro')\n",
        "  hamming = hamming_loss(y_true, y_pred)\n",
        "\n",
        "  metrics = {\n",
        "      \"roc_auc\": roc_auc,\n",
        "      \"hamming_loss\": hamming,\n",
        "      \"f1\": f1\n",
        "  }\n",
        "\n",
        "  return metrics\n",
        "\n",
        "def compute_metrics(p:EvalPrediction):\n",
        "  preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
        "\n",
        "  result = multi_labels_metrics(predictions=preds,\n",
        "                                labels=p.label_ids)\n",
        "\n",
        "  return result\n",
        "# Training Arguments\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "args = TrainingArguments(\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    output_dir = './results',\n",
        "    num_train_epochs=5,\n",
        "    save_steps=1000,\n",
        "    save_total_limit=2\n",
        ")\n",
        "\n",
        "trainer = Trainer(model=model,\n",
        "                  args=args,\n",
        "                  train_dataset=train_dataset,\n",
        "                  eval_dataset = val_dataset,\n",
        "                  compute_metrics=compute_metrics)\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "RSZnVNt9N-Jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "Lk5shyKpOGU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"distilbert-finetuned-imdb-multi-label\")\n",
        "import pickle\n",
        "with open(\"multi-label-binarizer.pkl\", \"wb\") as f:\n",
        "  pickle.dump(multilabel, f)"
      ],
      "metadata": {
        "id": "F2n_Z8DPOKzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Carol Danvers gets her powers entangled with those of Kamala Khan and Monica Rambeau, forcing them to work together to save the universe.\"\n",
        "\n",
        "encoding = tokenizer(text, return_tensors='pt')\n",
        "encoding.to(trainer.model.device)\n",
        "\n",
        "outputs = trainer.model(**encoding)\n",
        "sigmoid = torch.nn.Sigmoid()\n",
        "probs = sigmoid(outputs.logits[0].cpu())\n",
        "preds = np.zeros(probs.shape)\n",
        "preds[np.where(probs>=0.3)] = 1\n",
        "\n",
        "multilabel.classes_\n",
        "\n",
        "multilabel.inverse_transform(preds.reshape(1,-1))"
      ],
      "metadata": {
        "id": "VW4zTb5ROOGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds.reshape(1,-1)\n"
      ],
      "metadata": {
        "id": "ksd1B8YCOToo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}